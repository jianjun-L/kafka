
# brokers集群
#sdk消息主题
kafka.topic=test_content_xxx
bootstrap.servers=172.16.207.252:9092,172.16.207.253:9092,172.16.207.254:9092
#即所有副本都同步到数据时send方法才返回, 以此来完全判断数据是否发送成功, 理论上来讲数据不会丢失.
# 0发出去就不管了;1 Leader写磁盘里就ok 和 all ISR都ok
kafka.producer.acks=all
#发送失败重试次数
kafka.producer.retries=1
#批处理条数：当多个记录被发送到同一个分区时，生产者会尝试将记录合并到更少的请求中。这有助于客户端和服务器的性能。
kafka.producer.batch.size=100
#批处理延迟时间上限：即1ms过后，不管是否达到批处理数，都直接发送一次请求
kafka.producer.linger.ms=1
#即32MB的批处理缓冲区
kafka.producer.buffer.memory=33554432
kafka.producer.key.serializer=org.apache.kafka.common.serialization.StringSerializer
kafka.producer.value.serializer=com.example.kafka.utils.DataMessagaSerializer

#消费者群组ID，发布-订阅模式，即如果一个生产者，多个消费者都要消费，那么需要定义自己的群组，同一群组内的消费者只有一个能消费到消息
kafka.consumer.group.id=test_content_group
#如果为true，消费者的偏移量将在后台定期提交。
kafka.consumer.enable.auto.commit=true
#如何设置为自动提交（enable.auto.commit=true），这里设置自动提交周期 ；默认是5秒
kafka.consumer.auto.commit.interval.ms=3000
#在使用Kafka的组管理时，用于检测消费者故障的超时
kafka.consumer.session.timeout.ms=15000
kafka.consumer.auto.offset.reset=earliest
#消费监听器容器并发数
kafka.consumer.concurrency=3
kafka.consumer.key.deserializer=org.apache.kafka.common.serialization.StringDeserializer
kafka.consumer.value.deserializer=com.example.kafka.utils.DataMessagaDeserializer
#20190603sangsk增加参数，设置客户端获取元数据的超时时间
kafka.producer.max.blocktime=5000